{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RAvila-bioeng/M.R.AI/blob/monica/Alzheimer's_ResNet_trunc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2PgnhJE236L",
        "outputId": "32164b98-6c44-45c8-f170-e2da23e24644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2025.11.12)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JG6ot0xX463v",
        "outputId": "70fd7646-de2a-4e5f-bf81-dac6c0de018d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Dataset URL: https://www.kaggle.com/datasets/ninadaithal/imagesoasis\n",
            "License(s): apache-2.0\n",
            "Downloading imagesoasis.zip to /content\n",
            " 99% 1.22G/1.23G [00:14<00:00, 25.4MB/s]\n",
            "100% 1.23G/1.23G [00:14<00:00, 92.2MB/s]\n",
            "Contenido del directorio actual: ['.config', 'imagesoasis.zip', 'Data', 'gdrive', 'sample_data']\n",
            "Contenido de 'Data' si existe: ['Non Demented', 'Very mild Dementia', 'Moderate Dementia', 'Mild Dementia']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Kaggle_API\"\n",
        "\n",
        "# Descargar dataset\n",
        "!kaggle datasets download -d ninadaithal/imagesoasis\n",
        "\n",
        "# Descomprimir\n",
        "!unzip -q imagesoasis.zip\n",
        "\n",
        "import os\n",
        "print(\"Contenido del directorio actual:\", os.listdir(\".\"))\n",
        "print(\"Contenido de 'Data' si existe:\", os.listdir(\"Data\") if os.path.exists(\"Data\") else \"No existe 'Data'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFn0XiTIcs7r",
        "outputId": "0071d005-9c8b-46f0-cfa3-d1c6fba4341c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases encontradas en Data: ['Non Demented', 'Very mild Dementia', 'Moderate Dementia', 'Mild Dementia']\n",
            "\n",
            "Procesando clase: Non Demented\n",
            "  Imágenes totales en Non Demented: 67222\n",
            "  Ejemplos de nombres: ['OAS1_0277_MR1_mpr-2_147.jpg', 'OAS1_0001_MR1_mpr-4_118.jpg', 'OAS1_0249_MR2_mpr-3_115.jpg', 'OAS1_0058_MR1_mpr-2_152.jpg', 'OAS1_0121_MR1_mpr-1_127.jpg']\n",
            "  Pacientes totales detectados: 266\n",
            "  Pacientes que vamos a conservar en Non Demented: 120\n",
            "  Imágenes copiadas en Non Demented (subset): 29951\n",
            "\n",
            "Procesando clase: Very mild Dementia\n",
            "  Copiando TODAS las imágenes (13725)...\n",
            "\n",
            "Procesando clase: Moderate Dementia\n",
            "  Copiando TODAS las imágenes (488)...\n",
            "\n",
            "Procesando clase: Mild Dementia\n",
            "  Copiando TODAS las imágenes (5002)...\n",
            "\n",
            "✅ Data_subset creado en: ./Data_subset\n",
            "Non Demented: 29951 imágenes en Data_subset\n",
            "Very mild Dementia: 13725 imágenes en Data_subset\n",
            "Moderate Dementia: 488 imágenes en Data_subset\n",
            "Mild Dementia: 5002 imágenes en Data_subset\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "original_dir = \"./Data\"\n",
        "assert os.path.exists(original_dir), \"No existe ./Data. Revisa la celda 1.\"\n",
        "\n",
        "subset_dir = \"./Data_subset\"\n",
        "\n",
        "# Empezar *limpio*\n",
        "if os.path.exists(subset_dir):\n",
        "    shutil.rmtree(subset_dir)\n",
        "os.makedirs(subset_dir, exist_ok=True)\n",
        "\n",
        "valid_exts = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')\n",
        "N_PATIENTS_NON_DEMENTED = 120  # nº máximo de pacientes sanos\n",
        "\n",
        "classes = [d for d in os.listdir(original_dir)\n",
        "           if os.path.isdir(os.path.join(original_dir, d))]\n",
        "print(\"Clases encontradas en Data:\", classes)\n",
        "\n",
        "for cls in classes:\n",
        "    src_cls_dir = os.path.join(original_dir, cls)\n",
        "    dst_cls_dir = os.path.join(subset_dir, cls)\n",
        "    os.makedirs(dst_cls_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"\\nProcesando clase: {cls}\")\n",
        "\n",
        "    if \"non\" not in cls.lower():   # todas menos Non Demented\n",
        "        imgs = [f for f in os.listdir(src_cls_dir)\n",
        "                if f.lower().endswith(valid_exts)]\n",
        "        print(f\"  Copiando TODAS las imágenes ({len(imgs)})...\")\n",
        "        for fname in imgs:\n",
        "            shutil.copy2(os.path.join(src_cls_dir, fname),\n",
        "                         os.path.join(dst_cls_dir, fname))\n",
        "\n",
        "    else:\n",
        "        # Non Demented: reducir por nº de pacientes\n",
        "        imgs = [f for f in os.listdir(src_cls_dir)\n",
        "                if f.lower().endswith(valid_exts)]\n",
        "        print(f\"  Imágenes totales en {cls}: {len(imgs)}\")\n",
        "        print(\"  Ejemplos de nombres:\", imgs[:5])\n",
        "\n",
        "        images_by_patient = defaultdict(list)\n",
        "        for fname in imgs:\n",
        "            parts = fname.split('_')\n",
        "            # OAS1_0097_MR1_mpr-3_127.jpg → paciente = \"OAS1_0097\"\n",
        "            patient_id = \"_\".join(parts[:2]) if len(parts) >= 2 else parts[0]\n",
        "            images_by_patient[patient_id].append(fname)\n",
        "\n",
        "        print(f\"  Pacientes totales detectados: {len(images_by_patient)}\")\n",
        "\n",
        "        patients = list(images_by_patient.keys())\n",
        "        n_keep = min(N_PATIENTS_NON_DEMENTED, len(patients))\n",
        "        selected_patients = random.sample(patients, n_keep)\n",
        "\n",
        "        print(f\"  Pacientes que vamos a conservar en {cls}: {n_keep}\")\n",
        "\n",
        "        count_imgs = 0\n",
        "        for pid in selected_patients:\n",
        "            for fname in images_by_patient[pid]:\n",
        "                shutil.copy2(os.path.join(src_cls_dir, fname),\n",
        "                             os.path.join(dst_cls_dir, fname))\n",
        "                count_imgs += 1\n",
        "\n",
        "        print(f\"  Imágenes copiadas en {cls} (subset): {count_imgs}\")\n",
        "\n",
        "print(\"\\n✅ Data_subset creado en:\", subset_dir)\n",
        "\n",
        "# Comprobar conteos\n",
        "for cls in os.listdir(subset_dir):\n",
        "    cls_path = os.path.join(subset_dir, cls)\n",
        "    if os.path.isdir(cls_path):\n",
        "        n = len([f for f in os.listdir(cls_path)\n",
        "                 if f.lower().endswith(valid_exts)])\n",
        "        print(f\"{cls}: {n} imágenes en Data_subset\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "6nYAj9QATL3I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e464f2f2-fde2-45d2-9d5b-7c2d8a3cdb79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logical classes: ['Demented', 'Non Demented']\n",
            "\n",
            "Clase 'Demented': 81 pacientes\n",
            "  Train: 56 pacientes\n",
            "  Val:   12 pacientes\n",
            "  Test:  13 pacientes\n",
            "\n",
            "Clase 'Non Demented': 120 pacientes\n",
            "  Train: 84 pacientes\n",
            "  Val:   18 pacientes\n",
            "  Test:  18 pacientes\n",
            "\n",
            "Copiado terminado. Comprobando número de imágenes por split y clase...\n",
            "\n",
            "Split: train\n",
            "  Demented: 13359 imágenes\n",
            "  Non Demented: 20496 imágenes\n",
            "\n",
            "Split: val\n",
            "  Demented: 2745 imágenes\n",
            "  Non Demented: 4880 imágenes\n",
            "\n",
            "Split: test\n",
            "  Demented: 3111 imágenes\n",
            "  Non Demented: 4575 imágenes\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "subset_dir = \"./Data_subset\"\n",
        "split_root = \"./Data_split_2cls\"\n",
        "\n",
        "valid_exts = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')\n",
        "\n",
        "# Mapa de clases originales → clase binaria\n",
        "CLASS_MAP = {\n",
        "    \"Non Demented\": \"Non Demented\",\n",
        "    \"Very mild Dementia\": \"Demented\",\n",
        "    \"Mild Dementia\": \"Demented\",\n",
        "    \"Moderate Dementia\": \"Demented\",\n",
        "}\n",
        "\n",
        "# Limpiar salida\n",
        "if os.path.exists(split_root):\n",
        "    shutil.rmtree(split_root)\n",
        "os.makedirs(split_root, exist_ok=True)\n",
        "\n",
        "logical_classes = sorted(set(CLASS_MAP.values()))\n",
        "print(\"Logical classes:\", logical_classes)\n",
        "\n",
        "# Crear estructura de carpetas\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    for cls in logical_classes:\n",
        "        os.makedirs(os.path.join(split_root, split, cls), exist_ok=True)\n",
        "\n",
        "# Agrupar imágenes por (clase lógica, paciente)\n",
        "class_patient_images = {cls: defaultdict(list) for cls in logical_classes}\n",
        "\n",
        "for orig_class in os.listdir(subset_dir):\n",
        "    orig_class_path = os.path.join(subset_dir, orig_class)\n",
        "    if not os.path.isdir(orig_class_path):\n",
        "        continue\n",
        "\n",
        "    if orig_class not in CLASS_MAP:\n",
        "        print(f\"Saltando clase desconocida: {orig_class}\")\n",
        "        continue\n",
        "\n",
        "    logical_class = CLASS_MAP[orig_class]\n",
        "\n",
        "    for fname in os.listdir(orig_class_path):\n",
        "        fpath = os.path.join(orig_class_path, fname)\n",
        "        if not os.path.isfile(fpath):\n",
        "            continue\n",
        "\n",
        "        if not fname.lower().endswith(valid_exts):\n",
        "            continue\n",
        "\n",
        "        parts = fname.split(\"_\")\n",
        "        if len(parts) >= 2 and parts[0].startswith(\"OAS1\"):\n",
        "            patient_id = parts[0] + \"_\" + parts[1]\n",
        "        else:\n",
        "            patient_id = parts[0]\n",
        "\n",
        "        class_patient_images[logical_class][patient_id].append(fpath)\n",
        "\n",
        "# Función de split por pacientes\n",
        "def split_patients(patients, train_ratio=0.7, val_ratio=0.15, seed=42):\n",
        "    random.seed(seed)\n",
        "    patients = list(patients)\n",
        "    random.shuffle(patients)\n",
        "\n",
        "    n = len(patients)\n",
        "    if n == 0:\n",
        "        return [], [], []\n",
        "\n",
        "    n_train = max(1, int(train_ratio * n))\n",
        "    n_val = max(1, int(val_ratio * n))\n",
        "    n_test = n - n_train - n_val\n",
        "\n",
        "    if n_test <= 0:\n",
        "        n_test = 1\n",
        "        if n_val > 1:\n",
        "            n_val -= 1\n",
        "        else:\n",
        "            n_train = max(1, n_train - 1)\n",
        "\n",
        "    train_patients = patients[:n_train]\n",
        "    val_patients = patients[n_train:n_train + n_val]\n",
        "    test_patients = patients[n_train + n_val:]\n",
        "\n",
        "    return train_patients, val_patients, test_patients\n",
        "\n",
        "# Hacer el split y copiar archivos\n",
        "for logical_class in logical_classes:\n",
        "    patients_dict = class_patient_images[logical_class]\n",
        "    patients = list(patients_dict.keys())\n",
        "    print(f\"\\nClase '{logical_class}': {len(patients)} pacientes\")\n",
        "\n",
        "    train_p, val_p, test_p = split_patients(patients)\n",
        "\n",
        "    print(f\"  Train: {len(train_p)} pacientes\")\n",
        "    print(f\"  Val:   {len(val_p)} pacientes\")\n",
        "    print(f\"  Test:  {len(test_p)} pacientes\")\n",
        "\n",
        "    for split_name, split_pat_list in [(\"train\", train_p),\n",
        "                                       (\"val\", val_p),\n",
        "                                       (\"test\", test_p)]:\n",
        "        for pid in split_pat_list:\n",
        "            for src_path in patients_dict[pid]:\n",
        "                dst_path = os.path.join(\n",
        "                    split_root,\n",
        "                    split_name,\n",
        "                    logical_class,\n",
        "                    os.path.basename(src_path),\n",
        "                )\n",
        "                shutil.copy2(src_path, dst_path)\n",
        "\n",
        "print(\"\\nCopiado terminado. Comprobando número de imágenes por split y clase...\\n\")\n",
        "\n",
        "for split in [\"train\", \"val\", \"test\"]:\n",
        "    print(f\"Split: {split}\")\n",
        "    for cls in logical_classes:\n",
        "        cls_dir = os.path.join(split_root, split, cls)\n",
        "        if not os.path.isdir(cls_dir):\n",
        "            print(f\"  {cls}: 0 imágenes (directorio no encontrado)\")\n",
        "            continue\n",
        "        n_images = sum(\n",
        "            1 for f in os.listdir(cls_dir)\n",
        "            if f.lower().endswith(valid_exts)\n",
        "        )\n",
        "        print(f\"  {cls}: {n_images} imágenes\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dOO6Ob-utSAM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe11296-71cb-45db-ce7f-b5d5177daf19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases (orden interno, truncado): ['Demented', 'Non Demented']\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "data_dir = \"./Data_split_2cls\"\n",
        "\n",
        "# TRANSFORMS TRUNCADOS (recorte central)\n",
        "train_tfms_trunc = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((256, 256)),           # primero hacemos todas 256x256\n",
        "    transforms.CenterCrop((160, 160)),       # ⟵ aquí truncas: te quedas con la parte central\n",
        "    transforms.RandomHorizontalFlip(),       # augment\n",
        "    transforms.RandomRotation(10),           # augment\n",
        "    transforms.Resize((224, 224)),           # redimensionas al tamaño estándar de ResNet\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "val_tfms_trunc = transforms.Compose([\n",
        "    transforms.Grayscale(num_output_channels=3),\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.CenterCrop((160, 160)),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Datasets con esos transforms\n",
        "train_ds_trunc = datasets.ImageFolder(root=f\"{data_dir}/train\", transform=train_tfms_trunc)\n",
        "val_ds_trunc   = datasets.ImageFolder(root=f\"{data_dir}/val\",   transform=val_tfms_trunc)\n",
        "test_ds_trunc  = datasets.ImageFolder(root=f\"{data_dir}/test\",  transform=val_tfms_trunc)\n",
        "\n",
        "print(\"Clases (orden interno, truncado):\", train_ds_trunc.classes)\n",
        "\n",
        "batch_size = 32\n",
        "train_loader_trunc = DataLoader(train_ds_trunc, batch_size=batch_size, shuffle=True,  num_workers=2)\n",
        "val_loader_trunc   = DataLoader(val_ds_trunc,   batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "test_loader_trunc  = DataLoader(test_ds_trunc,  batch_size=batch_size, shuffle=False, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1kmgakH-O1_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d994fb27-c2a6-4454-b5da-92a6c971ee2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imágenes por clase en TRAIN (truncado): Counter({1: 20496, 0: 13359})\n",
            "Índice → Clase: {0: 'Demented', 1: 'Non Demented'}\n",
            "Pesos de clase usados en la loss (truncado): [1.5342467 1.       ]\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "targets_trunc = [label for _, label in train_ds_trunc.samples]\n",
        "class_counts_trunc = Counter(targets_trunc)\n",
        "print(\"Imágenes por clase en TRAIN (truncado):\", class_counts_trunc)\n",
        "print(\"Índice → Clase:\", {i: c for i, c in enumerate(train_ds_trunc.classes)})\n",
        "\n",
        "num_classes_trunc = len(train_ds_trunc.classes)\n",
        "counts_trunc = np.array([class_counts_trunc[i] for i in range(num_classes_trunc)], dtype=np.float32)\n",
        "total_trunc = counts_trunc.sum()\n",
        "\n",
        "class_weights_trunc = total_trunc / (counts_trunc + 1e-6)\n",
        "class_weights_trunc = class_weights_trunc / class_weights_trunc.min()\n",
        "\n",
        "print(\"Pesos de clase usados en la loss (truncado):\", class_weights_trunc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "J3wVHwblPpH5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "113d2835-1302-4945-b4ec-ffe32fc82019"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Usando dispositivo: cuda\n",
            "Nueva cabeza de la ResNet (truncado):\n",
            "Sequential(\n",
            "  (0): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Dropout(p=0.3, inplace=False)\n",
            "  (3): Linear(in_features=128, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Usando dispositivo:\", device)\n",
        "\n",
        "resnet_trunc = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "in_features = resnet_trunc.fc.in_features\n",
        "resnet_trunc.fc = nn.Sequential(\n",
        "    nn.Linear(in_features, 128),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(128, num_classes_trunc)   # 2 clases: ['Demented', 'Non Demented']\n",
        ")\n",
        "\n",
        "resnet_trunc = resnet_trunc.to(device)\n",
        "print(\"Nueva cabeza de la ResNet (truncado):\")\n",
        "print(resnet_trunc.fc)\n",
        "\n",
        "class_weights_trunc_tensor = torch.tensor(class_weights_trunc, dtype=torch.float32).to(device)\n",
        "criterion_trunc = nn.CrossEntropyLoss(weight=class_weights_trunc_tensor)\n",
        "\n",
        "optimizer_trunc = optim.Adam(resnet_trunc.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Aq3h5aEJQfJN"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch_trunc(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = outputs.max(1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc  = correct / total\n",
        "    return epoch_loss, epoch_acc\n",
        "\n",
        "\n",
        "def eval_model_trunc(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct, total = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    epoch_acc  = correct / total\n",
        "    return epoch_loss, epoch_acc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpxXvbsgQh6P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dae37557-3406-43b9-c391-98a43c826104"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRUNC] Epoch 1/10 | train_loss=0.1826, train_acc=0.922 | val_loss=0.7750, val_acc=0.835\n",
            "[TRUNC] Epoch 2/10 | train_loss=0.0605, train_acc=0.978 | val_loss=0.9281, val_acc=0.838\n",
            "[TRUNC] Epoch 3/10 | train_loss=0.0354, train_acc=0.987 | val_loss=1.1952, val_acc=0.819\n",
            "[TRUNC] Epoch 4/10 | train_loss=0.0272, train_acc=0.990 | val_loss=1.1802, val_acc=0.844\n",
            "[TRUNC] Epoch 5/10 | train_loss=0.0235, train_acc=0.992 | val_loss=1.1012, val_acc=0.831\n",
            "[TRUNC] Epoch 6/10 | train_loss=0.0184, train_acc=0.993 | val_loss=1.4328, val_acc=0.834\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 10  # o 15 si ves que todavía mejora la val_acc\n",
        "\n",
        "best_val_acc_trunc = 0.0\n",
        "best_model_state_trunc = None\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train_one_epoch_trunc(resnet_trunc, train_loader_trunc, optimizer_trunc, criterion_trunc)\n",
        "    val_loss, val_acc     = eval_model_trunc(resnet_trunc, val_loader_trunc, criterion_trunc)\n",
        "\n",
        "    print(f\"[TRUNC] Epoch {epoch+1}/{num_epochs} | \"\n",
        "          f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f} | \"\n",
        "          f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}\")\n",
        "\n",
        "    if val_acc > best_val_acc_trunc:\n",
        "        best_val_acc_trunc = val_acc\n",
        "        best_model_state_trunc = resnet_trunc.state_dict().copy()\n",
        "\n",
        "print(\"Mejor val_acc (truncado):\", best_val_acc_trunc)\n",
        "\n",
        "if best_model_state_trunc is not None:\n",
        "    resnet_trunc.load_state_dict(best_model_state_trunc)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SgPoViOQnC6"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "test_loss_trunc, test_acc_trunc = eval_model_trunc(resnet_trunc, test_loader_trunc, criterion_trunc)\n",
        "print(f\"[TRUNC] Test loss: {test_loss_trunc:.4f}, Test accuracy: {test_acc_trunc:.3f}\")\n",
        "\n",
        "resnet_trunc.eval()\n",
        "all_labels_trunc = []\n",
        "all_preds_trunc  = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader_trunc:\n",
        "        images = images.to(device)\n",
        "        outputs = resnet_trunc(images)\n",
        "        _, preds = outputs.max(1)\n",
        "\n",
        "        all_labels_trunc.extend(labels.numpy())\n",
        "        all_preds_trunc.extend(preds.cpu().numpy())\n",
        "\n",
        "all_labels_trunc = np.array(all_labels_trunc)\n",
        "all_preds_trunc  = np.array(all_preds_trunc)\n",
        "\n",
        "cm_trunc = confusion_matrix(all_labels_trunc, all_preds_trunc)\n",
        "print(\"\\n[TRUNC] Matriz de confusión:\")\n",
        "print(cm_trunc)\n",
        "\n",
        "print(\"\\n[TRUNC] Reporte de clasificación:\")\n",
        "print(classification_report(all_labels_trunc, all_preds_trunc, target_names=train_ds_trunc.classes))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Elige qué modelo y loader quieres inspeccionar\n",
        "# Si estás usando el modelo \"normal\":\n",
        "model = resnet\n",
        "loader = test_loader\n",
        "class_names = train_ds.classes\n",
        "\n",
        "# Si quieres el modelo truncado, comenta lo de arriba y descomenta esto:\n",
        "# model = resnet_trunc\n",
        "# loader = test_loader_trunc\n",
        "# class_names = train_ds_trunc.classes\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# ==============================\n",
        "# Tomar un batch del test_loader\n",
        "# ==============================\n",
        "batch_iter = iter(loader)\n",
        "images, labels = next(batch_iter)         # images: [B, 3, H, W], labels: [B]\n",
        "\n",
        "images = images.to(device)\n",
        "labels = labels.to(device)\n",
        "\n",
        "# ==============================\n",
        "# Predicciones del modelo\n",
        "# ==============================\n",
        "with torch.no_grad():\n",
        "    outputs = model(images)              # logits [B, num_classes]\n",
        "    probs = torch.softmax(outputs, dim=1)  # probabilidades [B, num_classes]\n",
        "    preds = torch.argmax(probs, dim=1)     # índice de clase predicha [B]\n",
        "\n",
        "# Probabilidad de la clase predicha (para mostrar en el título)\n",
        "pred_probs = probs[torch.arange(probs.size(0)), preds]  # [B]\n",
        "\n",
        "# Pasar todo a CPU + numpy para poder usar matplotlib\n",
        "images_np = images.cpu().numpy()\n",
        "labels_np = labels.cpu().numpy()\n",
        "preds_np = preds.cpu().numpy()\n",
        "pred_probs_np = pred_probs.cpu().numpy()\n",
        "\n",
        "# ==============================\n",
        "# Función para \"des-normalizar\" imágenes y visualizarlas\n",
        "# ==============================\n",
        "# Recuerda que normalizaste con mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]\n",
        "mean = np.array([0.485, 0.456, 0.406]).reshape(1, 1, 3)\n",
        "std  = np.array([0.229, 0.224, 0.225]).reshape(1, 1, 3)\n",
        "\n",
        "n_imgs = min(8, images_np.shape[0])   # máximo 8 imágenes\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "for i in range(n_imgs):\n",
        "    plt.subplot(2, 4, i + 1)\n",
        "\n",
        "    # image: [3, H, W] → [H, W, 3]\n",
        "    img = np.transpose(images_np[i], (1, 2, 0))\n",
        "\n",
        "    # Des-normalizar: img * std + mean\n",
        "    img = img * std + mean\n",
        "    img = np.clip(img, 0, 1)   # aseguramos rango [0,1] para matplotlib\n",
        "\n",
        "    plt.imshow(img)\n",
        "\n",
        "    true_id = int(labels_np[i])\n",
        "    pred_id = int(preds_np[i])\n",
        "\n",
        "    true_label = class_names[true_id]\n",
        "    pred_label = class_names[pred_id]\n",
        "    prob_pred  = pred_probs_np[i]\n",
        "\n",
        "    plt.title(f\"T: {true_label}\\nP: {pred_label}\\nProb: {prob_pred:.2f}\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "-uD38Y1H3OU2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}